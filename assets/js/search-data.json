{
  
    
        "post0": {
            "title": "Big Idea 3 Lesson Overview",
            "content": "Stay Tuned . We haven’t done this one yet. .",
            "url": "https://srihitakott1213.github.io/DVASS/2023/04/26/lessonoverview.html",
            "relUrl": "/2023/04/26/lessonoverview.html",
            "date": " • Apr 26, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Lists and Iteration, Random Values and Simulations",
            "content": "(In your VSCode, run this ) . Introduction: Simulations . Simulations are models of real-world phenomena or systems that use mathematical algorithms and computer programs simulate the real behavior and aspects of the subject being modeled. . Simulations are most often used to model complex or time-consuming things that would be difficult to test in real life, such as modeling the spread of diseases in certain ecosystems or testing the functionality of a potential product before it is made. . In this lesson, we will be looking at lists, iteration, and random values through the lens of simulations. . Review: Lists and Iteration . In this section, I go over lists to set up for iteration. . Code plans: . # Unique concept: adding things to the list # Going over lists using the deck list # Unique concept: shuffling the list # Unique concept: getting rid of things with pop . Recursive Loops . Recursive loops are used throughout the Python blackjack model. This is seen with the interaction between the game_start() function and the play_again() function. . def game_start(): #after the game setup and outcome... play_again() def play_again(): if player_chips != 0: pa = input(&#39;Would you like to play again? (Input &quot;y&quot; for yes and &quot;n&quot; for no.)&#39;) if pa.lower() == &quot;y&quot;: game_start() #HERE #... . For Loops . For loops are used quite often throughout the simulation and are helpful to easily be able to repeat a set of conditions to everything inside of a list/collection of data or to repeat a process a certain number of times. . An example of both of these things are very conveniently seen in the Deck class function build(). It also shows loops being nested. . def build(self): for suit in [&quot;Spades&quot;, &quot;Clubs&quot;, &quot;Diamonds&quot;, &quot;Hearts&quot;]: for val in range(2, 15): self.cards.append(Card(suit, val)) . Going piece by piece: . for suit in [&quot;Spades&quot;, &quot;Clubs&quot;, &quot;Diamonds&quot;, &quot;Hearts&quot;]: . Whatever is mentioned below this will be repeated four times, once for each of the listed card suits. . for val in range(2, 15) . Here, the for loop is essentially doing the same thing as before, this time with a list of numbers starting with 2 and ending with 14. As a result, the function runs 13 times, each time with a new integer value that is assigned to the value of a card of a certain suit. . While Loops . While loops aren&#39;t used in the program, but they offer a different way to repeat a set of instructions in a program. The procedure below the while [condition] line will occur until the condition is made not true. . **Student Interation**: How could this `build` function be altered to function with a while loop within it? . def build(deck): for suit in [&quot;Spades&quot;, &quot;Clubs&quot;, &quot;Diamonds&quot;, &quot;Hearts&quot;]: for val in range(2, 15): #HINT: try replacing this function deck.append(Card(suit, val)) #HINT: you may want to make a variable i to be the numbers in the range # you could increment it each loop . While loops also alter an alternative way to loop a set of instructions forever, until a precise thing occurs to break the loop. See the code below. . import random i = 0 while True: i += 1 ch = random.randint(1, 11) if ch == 10: print(f&quot;It took {str(i)} random generations to get 10.&quot;) break . It took 49 random generations to get 10. . 49 random generations is a lot more than it would normally take, but it&#39;s important for code to be able to model unlikely, yet possible scenarios. Speaking of random values... . Random Values . Because unpredictable randomness occurs in the real world, it&#39;s important to have a way to represent it. Simulations are able to use randomization, which could be in the form of random number generation or other methods like shuffle. . Card decks are a great example of how random values can be used to represent real-world scenarios. In the card simulation, the random module&#39;s shuffle function is used to quite literally shuffle the deck, seen below. . def shuffle(self): random.shuffle(self.cards) . Often, random selection methods use functions like randint or randrange as ways to select certain indexes in lists, or might use the random numbers in some other way. . Without shuffling the card order of the deck, can you think of a way that the aforementioned random module functions could be used to get a random card from the deck? . import random s_deck = Deck().show() #the deck shuffle is a separate function, so this is unsorted rando = random.randint(0, len(s_deck)) #generating a random number to use as index randsel = s_deck[rando] #selecting a random card with the index s_deck.pop(rando) #getting rid of the selected card from the deck print(randsel) . Possible Student Assignment . I might make an assignment here for the student to make a simple simulation that is at least mostly accurate to its real-world scenario. It must use: . A random value | At least one list or similar data type (dictionary, set, etc.) | Two different types of iteration | . # (Concert attendance? Wind speeds? Interactions between subjects in large environments?) # Think about the sort of things that could be saved in lists, dictionaries, etc. # (Even better if you can take advantage of the specific features of multiple types of data sets!) # What kind of iteration happens in the real world? # What occurs repeatedly, even over a long period of time? # Keep in mind that the simulation doesn&#39;t have to model real time. # You could, for example, model the results of a disease spreading through a population in just one second. .",
            "url": "https://srihitakott1213.github.io/DVASS/listitrvsim",
            "relUrl": "/listitrvsim",
            "date": " • Apr 26, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "3.1-3.5 Lesson Topics",
            "content": "3.1: Variables and Assignments . What is a variable? . A variable is an abstracttion inside a program that can hold a value. . | It is important to know that using meaningful variable names helps with teh readability of program code and the understanding of what values are being represented by the variables . | . Examples of Variables: . Answer the following questions: . What is the variable shown in the code below? | Why do you think that value is a variable? | . alphabet = &quot;abcdefghijklmnopqrstuvwxyz&quot; alphabetList = [] for i in alphabet: alphabetList.append(i) print(alphabetList) . What about in this code block? What is the variable? | . n = 81 i = 3 while i &lt;= n: print(i) i+=13 . For the first code block the variable is &quot;alphabet&quot; because it is being assigned to a string that contains the alphabet (and also has a meaningful name whichc makes it easy for the user to know exactly what that variable means). And in the second code block the variable is &quot;n&quot; because it is being assigned to an integer. . What is an assignment? What does it mean to assign something? . An assignment is essentially setting a value or giving a value to a variable. | . Consider the code segment below: . What is num1 initially being assigned to? | Does num1 stay that same value or does it change? If it does change then why? | . num1 = 25 num2 = 15 num3 = 55 num2 = num3 num3 = num1 num1 = num2 print(num1) print(num2) print(num3) . 3.2: Data Abstraction . What is Data Abstraction? . Data abstractions manage complexity in programs by giving a collection of data a name without referencing the specific details of the representation. . | Some examples of Data Abstraction are lists and strings . | . Strings . What are strings? Strings are an ordered sequence of characters that may contain letters, numbers, and all other special characters | . | . Some examples of strings: . Words | Phrases | Sentences | ID numbers . | Why do you think it wouldn&#39;t make sense to consider a phone number, for example, as an integer instead of a string? . Because usually integers are something we can add and subtract and average, whereas it would make sense to average a phone number because don&#39;t really consider them as just a bunch of numbers. | . | . Lists: . What are Lists? Lists are an ordered sequence of elements, where each element is a variable | . | . Some examples of lists: . Playlist of songs | names of students in a class | contacts on your phone . | Each element of a string is referenced by an index (which is a number) and they generally start 0 but for the AP Exam it starts at 1. . AP Exam: 1,2,3,4 etc. | Python: 0,1,2,3 etc. | . | . How do lists Manage Complexity of a program? . We may not need as many variables. For example: One Variable that holds all students would be better than having a variable for EACH student | There can also be a list of test scores so if they need to be curved then the same calculation can be applied to the list (that has all the students) instead of doing the calculations one at a time | . | . Answer the following questions about the code block below: . What is an example of a string? Are there multiple? | What is an example of a list? | . # variable of type string name = &quot;Sri Kotturi&quot; print(&quot;name&quot;, name, type(name)) # variable of type integer age = 16 print(&quot;age&quot;, age, type(age)) # variable of type float score = 90.0 print(&quot;score&quot;, score, type(score)) print() # variable of type list (many values in one variable) langs = [&quot;Python&quot;, &quot;JavaScript&quot;, &quot;Java&quot;, &quot;Bash&quot;, &quot;html&quot;] print(&quot;langs&quot;, langs, type(langs)) print(&quot;- langs[2]&quot;, langs[2], type(langs[2])) print() # variable of type dictionary (a group of keys and values) person = { &quot;name&quot;: name, &quot;age&quot;: age, &quot;score&quot;: score, &quot;langs&quot;: langs } print(&quot;person&quot;, person, type(person)) print(&#39;- person[&quot;name&quot;]&#39;, person[&quot;name&quot;], type(person[&quot;name&quot;])) . 3.3: Mathematical Expression . What is an algorithm? . it is a finite set of instructions that accomplishes a specific task | . Sequencing . means that there is an order in which to do things | . Selection . Helps to choose two different outcomes based off of a decision that the programmer wants to make | . Iteration . Repeat something until the condition is met. (also refered to as repetition) | . What is the output of the cell below? What Mathematical Expressions do you see being used? . grade1 = 10 grade2 = grade1 average_grade = (grade1 + grade2)/ 2 print(average_grade) . The problem above is being done in a specific order. | . What is the value of num1, num2, and num3? Explain the result for at least one of the values? . num1 = 2 num2 = 4 num3 = 6 num1 = num2 + num3 num3 = num1 + 5 num2 = (num1 + num3)/5 print(num1) print(num2) print(num3) . Arithmetic Operators: . Addition: . a + b Ex. grade + 10 | . | Subtraction: . a - b Ex. 100 - pointsDeducted | . | Multiplication . a b Ex. base height | . | Division: . a / b Ex. sum / 28 | . | Modulus: . a MOD b Ex. 17 MOD 2 | (Used with a % and it is the remainder after division) | . | Just like in math, order of operations apply here :) . | . 3.4: Strings . What is a string? . ordered sequence of characters | . Some Examples of string procedures: . len(str) . which gives the length of the string (how many letters there are) . | len(&quot;happy&quot;) returns 5 . | . | concat(str1, str2) . which combines str1 and str2 into one word . | concat(&quot;key&quot;, &quot;board&quot;) returns &quot;keyboard&quot; . | . | substring(str1, start, length) . starts with a letter within a string and then the length starts with the starting letter and prints each consecutive letter afterwards for however long the length is . | substring(&quot;APCSPrinciples&quot;, 3, 6) returns &quot;CSPrin&quot; . | . | . 3.5: Boolean Expressions . A Boolean Value is either true or false. | . Relational Operators: . Equal to: . a = b Ex. num_students = 30 | . | Not Equal to: . a ≠ b Ex. count ≠ 10 | . | Greater Than: . a &gt; b Ex. grade &gt; 70 | . | Less Than: . a &lt; b Ex. high_score &lt; current_score | . | Greater Than or Equal to: . a ≥ b Ex. num_pets ≥ 0 | . | Less Than or Equal to: . a ≤ b Ex. tacos ≤ 3 | . | . Write a Boolean expression to determine if num1 odd: . num1 MOD 2 = 1 | . | Write a Boolean expression to check if the temperature outside is less than 90 degrees. . temp &lt; 90 | . | . These examples use the relational operators above to display boolean expressions. . NOT: . isRaining = False . result = NOT(isRaining) . isRaining currently is false but when you use the NOT operator, the value outputted is true. . AND: . grade = 85 . result = grade &gt; 70 AND grade ≤ 100 . if the grade is above 70 and grade is less than or equal to 100 aare both true, then the entire expression is true. Since 85 is greater than 70 that part of that condition is true, and since 85 is less than 100 that part is true so since both of those values are true, the overall value will output TRUE. . grade = 45 . result = grade &gt; 70 AND grade ≤ 100 . For this one, 45 is less than 100 so that condition is true, but 45 is not greater than 70 so that condition is false. since both conditions are not true, the output of this expression is FALSE. . BOTH CONDITIONS HAVE TO BE TRUE IN ORDER TO OUTPUT TRUE. IF ONE OR BOTH OR FALSE, THEN THE OUTPUT IS FALSE . OR: . score = 175 . high_score = 150 . lives = 2 . result = score &gt; high_score OR lives = 0 . Since one of these values equals true, the value that is outputted by this expression is true because it either the first value OR the second value that has to be true. . score = 100 . high_score = 150 . lives = 1 . result = score &gt; high_score OR lives = 0 . Since both expressions are false, the entire expression ends up being false. .",
            "url": "https://srihitakott1213.github.io/DVASS/bigidea3/lesson",
            "relUrl": "/bigidea3/lesson",
            "date": " • Apr 12, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Handwritten Detection using Tensor Flow",
            "content": "Here&#39;s an example of building a simple neural network using Python and Pandas to classify images of handwritten digits from the MNIST dataset. . This example solves the problem of classifying images of handwritten digits from the MNIST dataset. The goal is to build a neural network that can accurately predict the digit shown in the input image. The problem is a classic machine learning problem and is often used as a benchmark for evaluating different algorithms and models. The example uses a simple neural network with two fully connected layers to achieve high accuracy in classifying the digits. . import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Flatten # Load the MNIST dataset mnist = tf.keras.datasets.mnist (X_train, y_train), (X_test, y_test) = mnist.load_data() # Flatten the input images X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) # Normalize the data X_train = X_train.astype(&#39;float32&#39;) / 255 X_test = X_test.astype(&#39;float32&#39;) / 255 # Convert the target variable to one-hot encoded vectors y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) y_test = tf.keras.utils.to_categorical(y_test, num_classes=10) # Define the model architecture model = Sequential() model.add(Dense(128, activation=&#39;relu&#39;, input_shape=(784,))) model.add(Dense(10, activation=&#39;softmax&#39;)) # Compile the model model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) # Train the model model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test)) # Evaluate the model test_loss, test_acc = model.evaluate(X_test, y_test) print(&#39;Test accuracy:&#39;, test_acc) . 2023-03-18 11:21:39.682503: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11490434/11490434 [==============================] - 1s 0us/step Epoch 1/10 . 2023-03-18 11:22:01.472441: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. . 1875/1875 [==============================] - 4s 2ms/step - loss: 0.2629 - accuracy: 0.9255 - val_loss: 0.1331 - val_accuracy: 0.9611 Epoch 2/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1132 - accuracy: 0.9669 - val_loss: 0.1036 - val_accuracy: 0.9666 Epoch 3/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0782 - accuracy: 0.9762 - val_loss: 0.0863 - val_accuracy: 0.9743 Epoch 4/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0581 - accuracy: 0.9826 - val_loss: 0.0750 - val_accuracy: 0.9770 Epoch 5/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0469 - accuracy: 0.9851 - val_loss: 0.0760 - val_accuracy: 0.9770 Epoch 6/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0357 - accuracy: 0.9883 - val_loss: 0.0723 - val_accuracy: 0.9783 Epoch 7/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0282 - accuracy: 0.9910 - val_loss: 0.0798 - val_accuracy: 0.9766 Epoch 8/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0231 - accuracy: 0.9930 - val_loss: 0.0737 - val_accuracy: 0.9784 Epoch 9/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0880 - val_accuracy: 0.9760 Epoch 10/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.0156 - accuracy: 0.9953 - val_loss: 0.0764 - val_accuracy: 0.9785 313/313 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9785 Test accuracy: 0.9785000085830688 . Visualize how this neural network works. . Use the matplotlib library to display some of the input images and the corresponding predicted labels. This code will create a 5x5 grid of subplots, each containing an input image from the test set and its predicted label. The np.argmax function is used to find the index of the predicted label with the highest probability. . import matplotlib.pyplot as plt # Make predictions on the test set y_pred = model.predict(X_test) # Plot some of the input images and their predicted labels fig, axs = plt.subplots(5, 5, figsize=(10, 10)) axs = axs.ravel() for i in range(25): axs[i].imshow(X_test[i].reshape(28, 28), cmap=plt.cm.gray_r) axs[i].set_title(&#39;Predicted label = {}&#39;.format(np.argmax(y_pred[i]))) axs[i].axis(&#39;off&#39;) plt.show() . 313/313 [==============================] - 0s 942us/step . Resources . Lesson to expand on MNIST. . TensorFlow&#39;s official MNIST tutorial:TensorFlow is a popular open-source machine learning framework, and their MNIST tutorial provides a great introduction to building neural networks with TensorFlow. You can find the tutorial here: https://www.tensorflow.org/tutorials/keras/classification. Coursera&#39;s Deep Learning Specialization: This is a series of courses taught by Andrew Ng that provides a comprehensive introduction to deep learning. The second course in the series, &quot;Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization,&quot; covers the MNIST example in detail. You can find the course here: https://www.coursera.org/specializations/deep-learning. . Neural Networks and Deep Learning book: This is a free online book by Michael Nielsen that provides an introduction to neural networks and deep learning. The book includes a chapter on the MNIST example, which provides a detailed explanation of how the neural network works. You can find the book here: http://neuralnetworksanddeeplearning.com/chap1.html. . Fast.ai: This is a set of online courses and resources that provide an introduction to deep learning. The courses include hands-on coding assignments, including an assignment on the MNIST example. You can find the courses here: https://www.fast.ai/. .",
            "url": "https://srihitakott1213.github.io/DVASS/2023/03/18/PBL-neural.html",
            "relUrl": "/2023/03/18/PBL-neural.html",
            "date": " • Mar 18, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Unit 2.4b Using Programs with Data, SQL",
            "content": "Database Programming is Program with Data . Each Tri 2 Final Project should be an example of a Program with Data. . Prepare to use SQLite in common Imperative Technique . Explore SQLite Connect object to establish database connection- Explore SQLite Cursor Object to fetch data from a table within a database | . Schema of Users table in Sqlite.db . Uses PRAGMA statement to read schema. . Describe Schema, here is resource Resource- What is a database schema? . What is the purpose of identity Column in SQL database? | What is the purpose of a primary key in SQL database? | What are the Data Types in SQL table? | . import sqlite3 database = &#39;instance/sqlite.db&#39; # this is location of database def schema(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Fetch results of Schema results = cursor.execute(&quot;PRAGMA table_info(&#39;users&#39;)&quot;).fetchall() # Print the results for row in results: print(row) # Close the database connection conn.close() schema() . Reading Users table in Sqlite.db . Uses SQL SELECT statement to read data . What is a connection object? After you google it, what do you think it does? | Same for cursor object? | Look at conn object and cursor object in VSCode debugger. What attributes are in the object? | Is &quot;results&quot; an object? How do you know? | . import sqlite3 def read(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Execute a SELECT statement to retrieve data from a table results = cursor.execute(&#39;SELECT * FROM users&#39;).fetchall() # Print the results if len(results) == 0: print(&quot;Table is empty&quot;) else: for row in results: print(row) # Close the cursor and connection objects cursor.close() conn.close() read() . Create a new User in table in Sqlite.db . Uses SQL INSERT to add row . Compore create() in both SQL lessons. What is better or worse in the two implementations? | Explain purpose of SQL INSERT. Is this the same as User init? | . import sqlite3 def create(): name = input(&quot;Enter your name:&quot;) uid = input(&quot;Enter your user id:&quot;) password = input(&quot;Enter your password&quot;) dob = input(&quot;Enter your date of birth &#39;YYYY-MM-DD&#39;&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to insert data into a table cursor.execute(&quot;INSERT INTO users (_name, _uid, _password, _dob) VALUES (?, ?, ?, ?)&quot;, (name, uid, password, dob)) # Commit the changes to the database conn.commit() print(f&quot;A new user record {uid} has been created&quot;) except sqlite3.Error as error: print(&quot;Error while executing the INSERT:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #create() . Updating a User in table in Sqlite.db . Uses SQL UPDATE to modify password . What does the hacked part do? | Explain try/except, when would except occur? | What code seems to be repeated in each of these examples to point, why is it repeated? | . import sqlite3 def update(): uid = input(&quot;Enter user id to update&quot;) password = input(&quot;Enter updated password&quot;) if len(password) &lt; 2: message = &quot;hacked&quot; password = &#39;gothackednewpassword123&#39; else: message = &quot;successfully updated&quot; # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to update data in a table cursor.execute(&quot;UPDATE users SET _password = ? WHERE _uid = ?&quot;, (password, uid)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: print(f&quot;The row with user id {uid} the password has been {message}&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the UPDATE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #update() . Delete a User in table in Sqlite.db . Uses a delete function to remove a user based on a user input of the id. . Is DELETE a dangerous operation? Why? | In the print statemements, what is the &quot;f&quot; and what does {uid} do? | . import sqlite3 def delete(): uid = input(&quot;Enter user id to delete&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: cursor.execute(&quot;DELETE FROM users WHERE _uid = ?&quot;, (uid,)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: # The uid was found in the table and the row was deleted print(f&quot;The row with uid {uid} was successfully deleted&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the DELETE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #delete() . Menu Interface to CRUD operations . CRUD and Schema interactions from one location by running menu. Observe input at the top of VSCode, observe output underneath code cell. . Why does the menu repeat? | Could you refactor this menu? Make it work with a List? | . def menu(): operation = input(&quot;Enter: (C)reate (R)ead (U)pdate or (D)elete or (S)chema&quot;) if operation.lower() == &#39;c&#39;: create() elif operation.lower() == &#39;r&#39;: read() elif operation.lower() == &#39;u&#39;: update() elif operation.lower() == &#39;d&#39;: delete() elif operation.lower() == &#39;s&#39;: schema() elif len(operation)==0: # Escape Key return else: print(&quot;Please enter c, r, u, or d&quot;) menu() # recursion, repeat menu try: menu() # start menu except: print(&quot;Perform Jupyter &#39;Run All&#39; prior to starting menu&quot;) . Hacks . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In this implementation, do you see procedural abstraction? | In 2.4a or 2.4b lecture Do you see data abstraction? Complement this with Debugging example. | Use Imperative or OOP style to Create a new Table or do something that applies to your CPT project. | . | . Reference... sqlite documentation .",
            "url": "https://srihitakott1213.github.io/DVASS/2023/03/16/AP-unit2-4b.html",
            "relUrl": "/2023/03/16/AP-unit2-4b.html",
            "date": " • Mar 16, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Files To Get . Save this file to your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/2023-03-09-AP-unit2-3.ipynb . Save these files into a subfolder named files in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/data.csv . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/grade.json . Save this image into a subfolder named images in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/images/table_dataframe.png . Pandas and DataFrames . In this lesson we will be exploring data analysis using Pandas. . College Board talks about ideas like Tools. &quot;the ability to process data depends on users capabilities and their tools&quot; | Combining Data. &quot;combine county data sets&quot; | Status on Data&quot;determining the artist with the greatest attendance during a particular month&quot; | Data poses challenge. &quot;the need to clean data&quot;, &quot;incomplete data&quot; | . | . From Pandas Overview -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame. | . . &#39;&#39;&#39;Pandas is used to gather data sets through its DataFrames implementation&#39;&#39;&#39; import pandas as pd . Cleaning Data . When looking at a data set, check to see what data needs to be cleaned. Examples include: . Missing Data Points | Invalid Data | Inaccurate Data | . Run the following code to see what needs to be cleaned . df = pd.read_json(&#39;files/grade.json&#39;) print(df) # What part of the data set needs to be cleaned? # From PBL learning, what is a good time to clean data? Hint, remember Garbage in, Garbage out? . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 2 578 12 2.78 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 6 145 12 2.95 7 167 10 3.90 8 235 9th Grade 3.15 9 nil 9 2.80 10 469 11 3.45 11 456 10 2.75 . Extracting Info . Take a look at some features that the Pandas library has that extracts info from the dataset . DataFrame Extract Column . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . DataFrame Sort . print(df.sort_values(by=[&#39;GPA&#39;])) print() #sort the values in reverse order print(df.sort_values(by=[&#39;GPA&#39;], ascending=False)) . Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 Student ID Year in School GPA 4 324 Junior 4.75 1 246 10 4.00 7 167 10 3.90 0 123 12 3.57 3 469 11 3.45 10 469 11 3.45 5 313 20 3.33 8 235 9th Grade 3.15 6 145 12 2.95 9 nil 9 2.80 2 578 12 2.78 11 456 10 2.75 . DataFrame Selection or Filter . print(df[df.GPA &gt; 3.00]) . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 7 167 10 3.90 8 235 9th Grade 3.15 10 469 11 3.45 . DataFrame Selection Max and Min . print(df[df.GPA == df.GPA.max()]) print() print(df[df.GPA == df.GPA.min()]) . Student ID Year in School GPA 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 . Create your own DataFrame . Using Pandas allows you to create your own DataFrame in Python. . Python Dictionary to Pandas DataFrame . import pandas as pd #the data can be stored as a python dictionary dict = { &quot;calories&quot;: [420, 380, 390], &quot;duration&quot;: [50, 40, 45] } #stores the data in a data frame print(&quot;-Dict_to_DF&quot;) df = pd.DataFrame(dict) print(df) print(&quot;-Dict_to_DF_labels--&quot;) #or with the index argument, you can label rows. df = pd.DataFrame(dict, index = [&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;]) print(df) . -Dict_to_DF calories duration 0 420 50 1 380 40 2 390 45 -Dict_to_DF_labels-- calories duration day1 420 50 day2 380 40 day3 390 45 . Examine DataFrame Rows . print(&quot;-Examine Selected Rows&quot;) #use a list for multiple labels: print(df.loc[[&quot;day1&quot;, &quot;day3&quot;]]) #refer to the row index: print(&quot;--Examine Single Row--&quot;) print(df.loc[&quot;day1&quot;]) . -Examine Selected Rows calories duration day1 420 50 day3 390 45 --Examine Single Row-- calories 420 duration 50 Name: day1, dtype: int64 . Pandas DataFrame Information . print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 3 entries, day1 to day3 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 calories 3 non-null int64 1 duration 3 non-null int64 dtypes: int64(2) memory usage: 180.0+ bytes None . Example of larger data set . Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/data.csv&#39;).sort_values(by=[&#39;Duration&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) . --Duration Top 10 Duration Pulse Maxpulse Calories 69 300 108 143 1500.2 79 270 100 131 1729.0 109 210 137 184 1860.4 60 210 108 160 1376.0 106 180 90 120 800.3 90 180 101 127 600.1 65 180 90 130 800.4 61 160 110 137 1034.4 62 160 109 135 853.0 67 150 107 130 816.0 --Duration Bottom 10 Duration Pulse Maxpulse Calories 68 20 106 136 110.4 100 20 95 112 77.7 89 20 83 107 50.3 135 20 136 156 189.0 94 20 150 171 127.4 95 20 151 168 229.4 139 20 141 162 222.4 64 20 110 130 131.4 112 15 124 139 124.2 93 15 80 100 50.5 . APIs are a Source for Writing Programs with Data . 3rd Party APIs are a great source for creating Pandas Data Frames. . Data can be fetched and resulting json can be placed into a Data Frame | Observe output, this looks very similar to a Database | . &#39;&#39;&#39;Pandas can be used to analyze data&#39;&#39;&#39; import pandas as pd import requests def fetch(): &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot; fetch = requests.get(url) json = fetch.json() # filter data for requirement df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths fetch() . country_name cases deaths 0 USA 82,649,779 1,018,316 1 India 43,057,545 522,193 2 Brazil 30,345,654 662,663 3 France 28,244,977 145,020 4 Germany 24,109,433 134,624 5 UK 21,933,206 173,352 . Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Amazing. . | When choosing a data set, think about the following... . Does it have a good sample size? | Is there bias in the data? | Does the data set need to be cleaned? | What is the purpose of the data set? | ... | . | Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) . | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. . Here are some suggestions... . Titanic Dataset:This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. | Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | . These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset... . Kaggle:The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. | UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . Titanic Data . Look at a sample of data. . import seaborn as sns # Load the titanic dataset titanic_data = sns.load_dataset(&#39;titanic&#39;) print(&quot;Titanic Data&quot;) print(titanic_data.columns) # titanic data set print(titanic_data[[&#39;survived&#39;,&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;class&#39;, &#39;fare&#39;, &#39;embark_town&#39;]]) # look at selected columns . Titanic Data Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;embarked&#39;, &#39;class&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;deck&#39;, &#39;embark_town&#39;, &#39;alive&#39;, &#39;alone&#39;], dtype=&#39;object&#39;) survived pclass sex age sibsp parch class fare 0 0 3 male 22.0 1 0 Third 7.2500 1 1 1 female 38.0 1 0 First 71.2833 2 1 3 female 26.0 0 0 Third 7.9250 3 1 1 female 35.0 1 0 First 53.1000 4 0 3 male 35.0 0 0 Third 8.0500 .. ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 Second 13.0000 887 1 1 female 19.0 0 0 First 30.0000 888 0 3 female NaN 1 2 Third 23.4500 889 1 1 male 26.0 0 0 First 30.0000 890 0 3 male 32.0 0 0 Third 7.7500 embark_town 0 Southampton 1 Cherbourg 2 Southampton 3 Southampton 4 Southampton .. ... 886 Southampton 887 Southampton 888 Southampton 889 Cherbourg 890 Queenstown [891 rows x 9 columns] . Use Pandas to clean the data. Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format. This is called &#39;Training&#39; or &#39;Cleaning&#39; data. . # Preprocess the data from sklearn.preprocessing import OneHotEncoder td = titanic_data td.drop([&#39;alive&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;class&#39;, &#39;embark_town&#39;, &#39;deck&#39;], axis=1, inplace=True) td.dropna(inplace=True) td[&#39;sex&#39;] = td[&#39;sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) td[&#39;alone&#39;] = td[&#39;alone&#39;].apply(lambda x: 1 if x == True else 0) # Encode categorical variables enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;) enc.fit(td[[&#39;embarked&#39;]]) onehot = enc.transform(td[[&#39;embarked&#39;]]).toarray() cols = [&#39;embarked_&#39; + val for val in enc.categories_[0]] td[cols] = pd.DataFrame(onehot) td.drop([&#39;embarked&#39;], axis=1, inplace=True) td.dropna(inplace=True) print(td) . survived pclass sex age sibsp parch fare alone embarked_C 0 0 3 1 22.0 1 0 7.2500 0 0.0 1 1 1 0 38.0 1 0 71.2833 0 1.0 2 1 3 0 26.0 0 0 7.9250 1 0.0 3 1 1 0 35.0 1 0 53.1000 0 0.0 4 0 3 1 35.0 0 0 8.0500 1 0.0 .. ... ... ... ... ... ... ... ... ... 705 0 2 1 39.0 0 0 26.0000 1 0.0 706 1 2 0 45.0 0 0 13.5000 1 0.0 707 1 1 1 42.0 0 0 26.2875 1 0.0 708 1 1 0 22.0 0 0 151.5500 1 0.0 710 1 1 0 24.0 0 0 49.5042 1 1.0 embarked_Q embarked_S 0 0.0 1.0 1 0.0 0.0 2 0.0 1.0 3 0.0 1.0 4 0.0 1.0 .. ... ... 705 0.0 1.0 706 0.0 1.0 707 1.0 0.0 708 0.0 1.0 710 0.0 0.0 [564 rows x 11 columns] . The result of &#39;Training&#39; data is making it easier to analyze or make conclusions. In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival. . This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person&#39;s chances of survival, and using that information to make predictions about whether an individual would have survived or not. . Data description:- Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file. - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) . Name - Name | Sex - Sex | Age - Age | Sibsp - Number of Siblings/Spouses Aboard | Parch - Number of Parents/Children Aboard | Ticket - Ticket Number | Fare - Passenger Fare | Cabin - Cabin | Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) | . | Perished Mean/Average . | . print(titanic_data.query(&quot;survived == 0&quot;).mean()) . survived 0.000000 pclass 2.464072 sex 0.844311 age 31.073353 sibsp 0.562874 parch 0.398204 fare 24.835902 alone 0.616766 embarked_C 0.185629 embarked_Q 0.038922 embarked_S 0.775449 dtype: float64 . Survived Mean/Average | . print(td.query(&quot;survived == 1&quot;).mean()) . survived 1.000000 pclass 1.878261 sex 0.326087 age 28.481522 sibsp 0.504348 parch 0.508696 fare 50.188806 alone 0.456522 embarked_C 0.152174 embarked_Q 0.034783 embarked_S 0.813043 dtype: float64 . Survived Max and Min Stats . print(td.query(&quot;survived == 1&quot;).max()) print(td.query(&quot;survived == 1&quot;).min()) . survived 1.0000 pclass 3.0000 sex 1.0000 age 80.0000 sibsp 4.0000 parch 5.0000 fare 512.3292 alone 1.0000 embarked_C 1.0000 embarked_Q 1.0000 embarked_S 1.0000 dtype: float64 survived 1.00 pclass 1.00 sex 0.00 age 0.75 sibsp 0.00 parch 0.00 fare 0.00 alone 0.00 embarked_C 0.00 embarked_Q 0.00 embarked_S 0.00 dtype: float64 . Machine Learning Visit Tutorials Point . Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. . Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn&#39;t seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others. . | Code Below prepares data for further analysis and provides an Accuracy. IMO, you would insert a new passenger and predict survival. Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down. . Decision Trees, prediction by a piecewise constant approximation. | Logistic Regression, the probabilities describing the possible outcomes. | . | . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Split arrays or matrices into random train and test subsets. X = td.drop(&#39;survived&#39;, axis=1) y = td[&#39;survived&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a decision tree classifier dt = DecisionTreeClassifier() dt.fit(X_train, y_train) # Test the model y_pred = dt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;DecisionTreeClassifier Accuracy:&#39;, accuracy) # Train a logistic regression model logreg = LogisticRegression() logreg.fit(X_train, y_train) # Test the model y_pred = logreg.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;LogisticRegression Accuracy:&#39;, accuracy) . DecisionTreeClassifier Accuracy: 0.7705882352941177 LogisticRegression Accuracy: 0.788235294117647 . /Users/johnmortensen/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( .",
            "url": "https://srihitakott1213.github.io/DVASS/2023/03/09/AP-unit2-3.html",
            "relUrl": "/2023/03/09/AP-unit2-3.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post6": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. . Clouds Impression | Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab | Size, height and width, number of pixels | Visual perception, lossy compression | . Displaying images in Python Jupyter notebook . Python Libraries and Concepts used for Jupyter and Files/Directories . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files? | What are the command you use in Windows terminal to access files? | What are some of the major differences? | . Provide what you observed, struggled with, or leaned while playing with this code. . Why is path a big deal when working with images? | How does the meta data source and label relate to Unit 5 topics? | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal? . Translate first 3 letters of your name to Base64. | . numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering? | From your consumer experience, what effects have you experienced from buffering? | How do these effects apply to images? | . Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed? | Describe Grey Scale algorithm in English or Pseudo code? | Describe scale image? What is before and after on pixels in three images? | Is scale image a type of compression? If so, line it up with College Board terms described? | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL | numpy | base64 | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.2 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | .",
            "url": "https://srihitakott1213.github.io/DVASS/2023/03/07/vivian.html",
            "relUrl": "/2023/03/07/vivian.html",
            "date": " • Mar 7, 2023"
        }
        
    
  

  
  
      ,"page0": {
          "title": "About Me",
          "content": "This website is powered by APCSP 1. . A blog for computer science principles. &#8617; . |",
          "url": "https://srihitakott1213.github.io/DVASS/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "",
          "content": "Objectives . . This Conceptual Framework teaches College Boards 5 Big Ideas of AP Computer Science Principles, Python, JavaScript, and Web Site application development together in an integrated fashion. The purpose is to teach College Board requirements while adding languages, tools, frameworks, and methodologies that meet Career Technical Education (CTE) needs. The outcome is preparing the High School student for the AP Test and simultaneously building the skills required for Computer Science internships. . . Blogs .",
          "url": "https://srihitakott1213.github.io/DVASS/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://srihitakott1213.github.io/DVASS/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}